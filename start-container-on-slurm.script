#!/bin/bash
##
## This SLURM script should be submitted from shared storage that all SLURM nodes can reach!
##
## The usage of SLURM Heterogeneous groups allows for a more elegant way to distigues between master and slave resources
##
#SBATCH --partition=dgx1v32g
#SBATCH --distribution=arbitrary
#SBATCH --output=outfile-%J
##SBATCH --time=6000:00
#SBATCH --nodes=1
#SBATCH --nodelist=dgx1v-loki-23
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=0
#SBATCH --mem-per-cpu=3072
#SBATCH --ntasks-per-node=1
#SBATCH hetjob
#SBATCH --nodes=4
#SBATCH --nodelist=dgx1v-loki-[25,27,29]
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=2
#SBATCH --mem-per-cpu=3072
#SBATCH --ntasks-per-node=1

## Show ouput
set -eux

## JAR names and download URL's (update when required)
CUDF_URL="https://storage.googleapis.com/mirror_rapids_ej/cudf-0.18-20210112.093909-33-cuda11.jar"
CUDF_NAME="cudf-0.18-20210112.093909-33-cuda11.jar"
RAPIDS_SPARK_URL="https://storage.googleapis.com/mirror_rapids_ej/rapids-4-spark_2.12-0.4.0-20210112.085853-45.jar"
RAPIDS_SPARK_NAME="rapids-4-spark_2.12-0.4.0-20210112.085853-45.jar"
RAPIDS_SPARK_SQL_URL="https://storage.googleapis.com/mirror_rapids_ej/rapids-4-spark-sql_2.12-0.4.0-20210112.085254-46.jar"
RAPIDS_SPARK_SQL_NAME="rapids-4-spark-sql_2.12-0.4.0-20210112.085254-46.jar"
RAPIDS_TESTS_URL="https://storage.googleapis.com/mirror_rapids_ej/rapids-4-spark-integration-tests_2.12-0.4.0-20210112.090812-45-jar-with-dependencies.jar"
RAPIDS_TESTS_NAME="rapids-4-spark-integration-tests_2.12-0.4.0-20210112.090812-45-jar-with-dependencies.jar"

## Mountpoint for shared filesystem (required for config)
export MOUNT="/home/ejansen"

## Enable/Disable data set generation.
DATAGEN="enable"

## TPCDS file settings
INPUT_PATH="file://${MOUNT}/input/useDecimal=false,useDate=true,filterNull=false"
OUTPUT_PATH="file://${MOUNT}/output"

## TPCDS s3 settings
#INPUT_PATH="gs://ec-benchmark-data/tpc-ds/tpcds_sf3000-parquet/useDecimal=false,useDate=true,filterNull=false"
#OUTPUT_PATH="file://${MOUNT}/output"
S3_ENDPOINT="https://storage.googleapis.com"
S3A_CREDS_USR=""
S3A_CREDS_PSW=""

## TPCDS format settings
INPUT_FORMAT="parquet"
OUTPUT_FORMAT="parquet"

## TPCDS test settings
SCALEFACTOR=1
BENCHMARK="tpcds"
ITERATIONS=1
QUERY="q1"

## Enable or disable GPU with "true" or "false"
ENABLE_GPU="true"

## Set threads per GPU (1)
CONCURRENTGPU=2

## Set shuffle.partitions, depending on the dataset size: 1T=1048576M / 200 = 5242 but cuDF only supports 2147483647 rows which will become a problem
## Set shuffle.partitions, depending on the dataset size: 3T=3145728M / 1024 = 3072
## Set shuffle.partitions, depending on the dataset size: 3T=3145728M / 512 = 6144
## Set shuffle.partitions, depending on the number of cores: 64

SHUFFLE_PARTITIONS=64

## Set Spark SQL partition size ("128M")
MAXPARTITIONBYTES="256M"

## Configure driver memory ("10240M")
DRIVER_MEMORY="10240M"

## Set SPILL to storage size ("16384M")
SPILL_STORAGE_SIZE="16384M"

## Set RAPIDS number of read threads
MULTI_THREADED_READ=$(( ${SLURM_CPUS_PER_TASK_HET_GROUP_1} * ${SLURM_JOB_NUM_NODES_HET_GROUP_1} / 2 ))

##*** No manual input of variables beyond this point ***##
##########################################################

## Set Spark master
MASTER=`hostname`

## Set TPCDS home
export TPCDS_HOME="${MOUNT}/tpcds"

sudo mkdir -p ${MOUNT}/conf
sudo chown -R $(id -u):$(id -g) ${MOUNT}/conf
sudo cp wait-worker.sh ${MOUNT}/conf/wait-worker.sh
sudo chmod +x ${MOUNT}/conf/wait-worker.sh
sudo cp kill* ${MOUNT}/conf
sudo chmod +x ${MOUNT}/conf/kill*.sh
sudo cp -r tpcds ${MOUNT}/
sudo chown -R $(id -u):$(id -g) ${MOUNT}/tpcds
sudo chmod +x ${MOUNT}/tpcds/datagen.sh
sudo chmod +x ${MOUNT}/tpcds/run-tpcds.sh
sudo chmod +x ${MOUNT}/tpcds/tpcds-kit/tools/dsdgen
sudo cp -r sparkRapidsPlugin ${MOUNT}/
sudo chown -R $(id -u):$(id -g) ${MOUNT}/sparkRapidsPlugin
sudo mkdir -p ${MOUNT}/history
sudo chown -R $(id -u):$(id -g) ${MOUNT}/history
sudo mkdir -p ${MOUNT}/spark-warehouse
sudo chown -R $(id -u):$(id -g) ${MOUNT}/spark-warehouse

## Set RAPIDS dir name
SPARK_RAPIDS_DIR="${MOUNT}/sparkRapidsPlugin"

## Set SPARK CUDF JAR
if [ ! -f "${SPARK_RAPIDS_DIR}/${CUDF_NAME}" ]
then
    wget -P ${SPARK_RAPIDS_DIR} -c ${CUDF_URL}
else
    echo "${CUDF_NAME} exists"
fi
SPARK_CUDF_JAR="${SPARK_RAPIDS_DIR}/${CUDF_NAME}"

## Set RAPIDS 4 SPARK JAR
if [ ! -f "${SPARK_RAPIDS_DIR}/${RAPIDS_SPARK_NAME}" ]
then
    wget -P ${SPARK_RAPIDS_DIR} -c ${RAPIDS_SPARK_URL}
else
    echo "${RAPIDS_SPARK_NAME} exists"
fi
SPARK_RAPIDS_PLUGIN_JAR="${SPARK_RAPIDS_DIR}/${RAPIDS_SPARK_NAME}"

## Set RAPIDS test JAR
if [ ! -f "${TPCDS_HOME}/${RAPIDS_TESTS_NAME}" ]
then
    wget -P ${TPCDS_HOME} -c ${RAPIDS_TESTS_URL}
else
    echo "${RAPIDS_TESTS_NAME} exists"
fi
SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR="${TPCDS_HOME}/${RAPIDS_TESTS_NAME}"

## Set SPARK SQL JAR
if [ ! -f "${TPCDS_HOME}/${RAPIDS_SPARK_SQL_NAME}" ]
then
    wget -P ${TPCDS_HOME} -c ${RAPIDS_SPARK_SQL_URL}
else
    echo "${RAPIDS_SPARK_SQL_NAME} exists"
fi
SPARK_SQL_JAR="${TPCDS_HOME}/${RAPIDS_SPARK_SQL_NAME}"

TOTAL_CORES=$(( ${SLURM_CPUS_PER_TASK_HET_GROUP_1} * ${SLURM_JOB_NUM_NODES_HET_GROUP_1} ))

if [ ${ENABLE_GPU} = "false" ]
  then 
       GPU_PER_NODE=0
       NUM_EXECUTORS=${SLURM_JOB_NUM_NODES_HET_GROUP_1}
       RESOURCE_GPU_AMT=0
       WORKER_OPTS=""
       CONCURRENTGPU=1
       NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))
  else 
       GPU_PER_NODE=${SLURM_GPUS_PER_NODE_HET_GROUP_1}
       NUM_EXECUTORS=$(( ${SLURM_GPUS_PER_NODE_HET_GROUP_1} * ${SLURM_JOB_NUM_NODES_HET_GROUP_1} ))
       NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))
       RESOURCE_GPU_AMT=$( awk -v e="${NUM_EXECUTORS}" -v c="${TOTAL_CORES}" 'BEGIN { printf "%s", e/c }' </dev/null )
       WORKER_OPTS="-Dspark.worker.resource.gpu.amount=${GPU_PER_NODE} -Dspark.worker.resource.gpu.discoveryScript=${MOUNT}/sparkRapidsPlugin/getGpusResources.sh"
fi

##PATH=${PATH}:${SPARK_HOME}/bin:${SPARK_HOME}/sbin
EXECUTOR_MEMORY=$(( ${TOTAL_CORES} * ${SLURM_MEM_PER_CPU_HET_GROUP_1} / ${NUM_EXECUTORS} ))
PINNED_POOL_SIZE=$(( ${EXECUTOR_MEMORY} / 4 / ${CONCURRENTGPU} ))M
EXECUTOR_MEMORY=${EXECUTOR_MEMORY}M 

## FOR DATAGEN ONLY BUT NEEDS ADDITIONAl WORK
##sed -i -e "s/__SF__/${SCALEFACTOR}/g" ${TPCDS_HOME}/tpcds_datagen.scala
##sed -i -e "s/__IP__/\/data\/tpcds-data-in/g" ${TPCDS_HOME}/tpcds_datagen.scala
##sed -i -e "s/__TH__/\/data\/tpcds/g" ${TPCDS_HOME}/tpcds_datagen.scala

srun --het-group=0 -w `hostname` ${MOUNT}/conf/kill-master.sh || true &&
srun --het-group=1 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_1}" ${MOUNT}/conf/kill-worker.sh || true &&
srun --het-group=0 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_0}" bash -c "echo -n 'Clearing cache on ' && sync && sudo /sbin/sysctl vm.drop_caches=3"
srun --het-group=1 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_1}" bash -c "echo -n 'Clearing cache on ' && sync && sudo /sbin/sysctl vm.drop_caches=3"

scontrol show hostname $SLURM_JOB_NODELIST_HET_GROUP_1 > ${MOUNT}/conf/slaves

conf=${MOUNT}/conf/spark-defaults.conf
echo "spark.default.parallelism" $(( ${NUM_EXECUTORS} )) > $conf
echo "spark.submit.deployMode" client >> $conf
echo "spark.master" spark://`hostname`:7077 >> $conf
echo "spark.executor.cores" ${NUM_EXECUTOR_CORES} >> $conf
echo "spark.executor.memory" ${EXECUTOR_MEMORY} >> $conf
echo "spark.eventLog.enabled" true >> $conf
echo "spark.eventLog.dir" file:${MOUNT}/history/ >> $conf
echo "spark.history.fs.logDirectory" file:${MOUNT}/history/ >> $conf

## Enable when existing image needs to be deleted
##srun --het-group=0 -n 1 -N 1 -w `hostname` docker rmi gcr.io/data-science-enterprise/spark-master-slurm:3.0.1 || true

srun --het-group=0 -n 1 -N 1 -w `hostname` docker run -dit \
-e MASTER="${MASTER}" \
-e ENABLE_GPU="${ENABLE_GPU}" \
-e SPARK_WORKER_CORES=`nproc` \
-e SPARK_WORKER_OPTS="${WORKER_OPTS}" \
-e CONCURRENTGPU="${CONCURRENTGPU}" \
-e TOTAL_CORES="${TOTAL_CORES}" \
-e NUM_EXECUTORS="${NUM_EXECUTORS}" \
-e NUM_EXECUTOR_CORES="${NUM_EXECUTOR_CORES}" \
-e EXECUTOR_MEMORY="${EXECUTOR_MEMORY}" \
-e PINNED_POOL_SIZE="${PINNED_POOL_SIZE}" \
-e DRIVER_MEMORY="${DRIVER_MEMORY}" \
-e SHUFFLE_PARTITIONS="${SHUFFLE_PARTITIONS}" \
-e MAXPARTITIONBYTES="${MAXPARTITIONBYTES}" \
-e SPILL_STORAGE_SIZE="${SPILL_STORAGE_SIZE}" \
-e S3A_CREDS_USR="${S3A_CREDS_USR}" \
-e S3A_CREDS_PSW="${S3A_CREDS_PSW}" \
-e S3_ENDPOINT="${S3_ENDPOINT}" \
-e OUTPUT_PATH="${OUTPUT_PATH}" \
-e INPUT_PATH="${INPUT_PATH}" \
-e INPUT_FORMAT="${INPUT_FORMAT}" \
-e OUTPUT_FORMAT="${OUTPUT_FORMAT}" \
-e SPARK_RAPIDS_PLUGIN_JAR="${SPARK_RAPIDS_PLUGIN_JAR}" \
-e SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR="${SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR}" \
-e SPARK_CUDF_JAR="${SPARK_CUDF_JAR}" \
-e BENCHMARK="${BENCHMARK}" \
-e ITERATIONS="${ITERATIONS}" \
-e QUERY="$QUERY" \
-e SCALEFACTOR="$SCALEFACTOR" \
-e TPCDS_HOME="${TPCDS_HOME}" \
-e RESOURCE_GPU_AMT="${RESOURCE_GPU_AMT}" \
-e MULTI_THREADED_READ="${MULTI_THREADED_READ}" \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}:/data \
-v /tmp:/tmp \
--network host \
--name master \
--rm \
gcr.io/data-science-enterprise/spark-master-slurm:3.0.1

## Enable when existing image needs to be deleted
##srun --het-group=1 --ntasks=${SLURM_JOB_NUM_NODES_HET_GROUP_1} --ntasks-per-node=1 docker rmi gcr.io/data-science-enterprise/spark-worker-slurm:3.0.1 || true

srun --het-group=1 --ntasks=${SLURM_JOB_NUM_NODES_HET_GROUP_1} --ntasks-per-node=1 docker run -dit \
-e MASTER=${MASTER} \
-e SPARK_WORKER_CORES=`nproc` \
-e SPARK_WORKER_OPTS="${WORKER_OPTS}" \
-e SPARK_RAPIDS_PLUGIN_JAR="${SPARK_RAPIDS_PLUGIN_JAR}" \
-e SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR="${SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR}" \
-e SPARK_CUDF_JAR="${SPARK_CUDF_JAR}" \
-e TPCDS_HOME="${TPCDS_HOME}" \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}:/data \
-v /tmp:/tmp \
--network host \
--name worker \
--rm \
gcr.io/data-science-enterprise/spark-worker-slurm:3.0.1

srun --het-group=0 -n 1 -N 1 -w `hostname` bash -c $MOUNT/conf/wait-worker.sh

echo "All workers registered!"

## Run data generation when enabled
if [ "$DATAGEN" == "true" ];
then
    srun --het-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c './datagen.sh'
fi

## Run for TPCDS Benchmark
srun --het-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c './run-tpcds.sh'

## Keep cluster alive for additional testing or review
#sleep infinity

## Cleanup (only works if sleep is disabled)
echo "testing complete, please check tpcds-benchmark-results-$SLURM_JOB_ID for relevent output data." 
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname`  ${MOUNT}/conf/kill-master.sh || true
srun --het-group=1 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_1}" ${MOUNT}/conf/kill-worker.sh || true
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mkdir tpcds-benchmark-results-$SLURM_JOB_ID
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mkdir tpcds-benchmark-results-$SLURM_JOB_ID/history
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mkdir tpcds-benchmark-results-$SLURM_JOB_ID/config
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mv outfile-$SLURM_JOB_ID tpcds-benchmark-results-${SLURM_JOB_ID}/
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mv ${TPCDS_HOME}/spark-submit-template.txt ${TPCDS_HOME}/tpcds-config.properties ${TPCDS_HOME}/run-tpcds.sh ${TPCDS_HOME}/benchmark.py tpcds-benchmark-results-${SLURM_JOB_ID}/config
mv ${TPCDS_HOME}/*.json tpcds-benchmark-results-${SLURM_JOB_ID}/
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname` sudo mv ${MOUNT}/history/* tpcds-benchmark-results-${SLURM_JOB_ID}/history

echo "Collection & Cleanup complete, bye bye.."
